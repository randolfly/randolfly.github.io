---
date: 2022-08-07
tag:
  - Math
  - math
  - mat
  - derivate
category:
  - library
  - 数学
  - 线性代数
---

# 矩阵求导

# 矩阵求导


## 理解矩阵求导

矩阵的求导在很多领域有所应用，比如控制论、优化等。这里对矩阵求导的主要内容进行分析和介绍，力图使得读者能够从更完善的角度理解矩阵求导。

这里，使用小写字母 $x$ 代表标量/向量（根据语境可以看出），大写字母 $X$ 代表矩阵。

### 宏观上理解矩阵求导

从定义上来看，标量对矩阵的导数可以定义为：

$$
\frac{\partial f}{\partial X}=\left[ \frac{\partial f}{\partial X_{ij}} \right]
$$

也就是 f 对 X 逐元素求导，同时保证结果和 X 相同纬度。这个定义对 X 的每个元素进行一样的操作，在实际应用上就会出现很大的问题。因为对于复杂函数以及后面介绍的矩阵对矩阵求导的情形，我们很难将每个元素拆分后进行求导。从更加本质的角度来看，我们实际上可以找一个不依赖于 **个体** 的方法，也就是针对矩阵整体进行分析。

我们考虑一元函数的导数：


$$
\begin{cases}
	\mathrm{d}f=f'\left( x \right) \mathrm{d}x\\
	\mathrm{d}f=\sum_i{\frac{\partial f}{\partial x_i}\mathrm{d}x_i}=\frac{\partial f}{\partial \boldsymbol{x}}^T\mathrm{d}\boldsymbol{x}\\
\end{cases}
$$


也就是说，我们可以从微分的角度定义导数。

::: note note
事实上，回忆一开始定义导数的时候，我们就是通过微分的角度来定义的，现在只是返回了更原始的定义罢了
:::


根据上面一维的情形，我们整理导数为：

::: info lemma
全微分$df$是梯度向量$\frac{\partial f}{\partial \boldsymbol{x}}\left( \mathrm{d}im: n\times 1 \right)$和微分向量$\mathrm{d}\boldsymbol{x}\left( \mathrm{d}im: n\times 1 \right)$的内积
:::

我们可以类似的将矩阵的导数和微分建立这样的联系：
::: tip definition
$$
\mathrm{d}f=\sum_{i=1}^m{\sum_{j=1}^n{\frac{\partial f}{\partial X_{ij}}\mathrm{d}X_{ij}}}=tr\left( {\color{red}\frac{\partial f}{\partial X}}^T\mathrm{d}X \right) 
$$

:::

^a57269

注意上面的红色代表我们想要求的矩阵导数，tr 代表矩阵的迹 (trace)，用来定义矩阵空间中的内积。(事实上，矩阵的一种内积定义方式就是 $\left< A,B \right> =tr\left( A^TB \right) =\sum_i{\sum_j{A_{ij}B_{ij}}}$)

得到了这样全微分关系是，我们需要考虑如何求解其中的每一项。首先是 $df$。

很普遍的一种思路是首先建立一些基本元素的微分关系，然后建立在一些法则下的微分运算法则，从何可以组合得到最终的微分结果，这里也采用这一思路。 ^bd1e3b

 - 基本运算 ^b44308
	 - 加减法
		- $\mathrm{d}\left( X\pm Y \right) =\mathrm{d}X\pm \mathrm{d}Y$
	- 矩阵乘法
		- $\mathrm{d}\left( XY \right) =\left( \mathrm{d}X \right) Y+X\left( \mathrm{d}Y \right)$
	- 转置
		- $\mathrm{d}\left( X^T \right) =\left( \mathrm{d}X \right) ^T$
	- 迹
		- $\mathrm{d}\left( \mathrm{tr}\left( X \right) \right) =\mathrm{tr}\left( \mathrm{d}\left( X \right) \right)$
	- 逆
		- $\mathrm{d}\left( X^{-1} \right) =-X^{-1}\mathrm{d}XX^{-1}$
		- 可以从 $XX^{-1}=I\Rightarrow \mathrm{d}X\left( X^{-1} \right) +X\mathrm{d}\left( X^{-1} \right) =0$ 得到
- 行列式
	- $\mathrm{d}\left| X \right|=\mathrm{tr}\left( X^{\#}\mathrm{d}X \right)$，其中 $X^{\#}$是X的伴随矩阵
	- 在 X 可逆时，有 $\mathrm{d}\left| X \right|=\left| X \right|\mathrm{tr}\left( X^{-1}\mathrm{d}X \right)$
	- 证明参考 [行列式微分形式的推导 - 知乎](https://zhuanlan.zhihu.com/p/144255438)
		- 伴随矩阵的元素满足 $X_{ji}^{*}=A_{ij}=\left( -1 \right) ^{i+j}\left| \hat{X}_{ij} \right|$，其中 $\hat{X}_{ij}$ 为矩阵 X 删掉第 i 行和第 j 列后的剩余矩阵。需要注意，由于伴随矩阵按照列布局代数余子式，因此 ${X}_{ij}^{*}$和$A_{ij}$ 的下标颠倒
		- 假定算符 $+_{ij}$ 代表对矩阵位置 (i, j) 中的元素增加某个量，根据偏导数定义有：
			- 
$$
\begin{array}{c}
	\frac{\partial \left| X \right|}{\partial X_{ij}}=\underset{\varepsilon \rightarrow 0}{\lim}\frac{\left| X+_{ij}\varepsilon \right|-\left| X \right|}{\varepsilon}\\
	=\underset{\varepsilon \rightarrow 0}{\lim}\frac{\left( X_{ij}+\varepsilon \right) A_{ij}+\sum_{k\ne j}{X_{ik}A_{ik}}-\sum_k{X_{ik}A_{ik}}}{\varepsilon}\\
	=\underset{\varepsilon \rightarrow 0}{\lim}\frac{\left( X_{ij}+\varepsilon \right) A_{ij}-X_{ij}A_{ij}}{\varepsilon}\\
	=\underset{\varepsilon \rightarrow 0}{\lim}\frac{\varepsilon A_{ij}}{\varepsilon}=A_{ij}=X_{ji}^{*}\\
\end{array}
$$

	- 逐元素乘法
		- $\mathrm{d}\left( X\odot Y \right) =\mathrm{d}X\odot Y+X\odot \mathrm{d}Y$
		- 其中 $\odot$ 代表尺寸相同的矩阵逐元素相乘
	- 逐元素函数
		- $\mathrm{d}\sigma \left( X \right) =\sigma '\left( X \right) \odot \mathrm{d}X$，其中 $\sigma \left( X \right) =\left[ \sigma \left( X_{ij} \right) \right]$ 是逐元素标量函数运算

### 标量对矩阵的求导

根据上面的思考，我们已经得到了
矩阵微分和求导的关系 [^a57269](./#^a57269) $df$ 的微分形式
[^b44308](./#^b44308)

从而下面需要做的就是得到右边的结果，这里只需要一些简单的迹技巧：
- 标量套上迹: $a=\mathrm{tr}\left( a \right)$
- 转置: $\mathrm{tr}\left( A^T \right) =\mathrm{tr}\left( A \right)$
- 线性: $\mathrm{tr}\left( A\pm B \right) =\mathrm{tr}\left( A \right) \pm \mathrm{tr}\left( B \right)$
- 矩阵乘法交换: $\mathrm{tr}\left( AB \right) =\mathrm{tr}\left( BA \right)$，其中 $A$ 和 $B^T$ 尺寸相同。
- 矩阵乘法/逐元素乘法交换: $\mathrm{tr}\left( A^T\left( B\odot C \right) \right) =\mathrm{tr}\left( \left( A\odot B \right) ^TC \right)$，其中两侧的元素都等于 $\sum_{i,j}{A_{ij}B_{ij}C_{ij}}$

通过上面的迹技巧，可以将 $df$ 转换为 $\mathrm{tr}\left( A^T\mathrm{d}X \right)$ 的形式，那么 $A$ 就是需要求的 $\frac{\partial f}{\partial X}$

需要注意这里还有最后一点需要注意，即 **复合函数**。在这里不能直接沿用微积分中的标量求导的链式法则，因为矩阵对矩阵的导数目前还没有定义。

这里从微分的角度入手建立复合法则。基本原则是：首先写出 $\mathrm{d}f=\mathrm{tr}\left( \frac{\partial f}{\partial Y}^T\mathrm{d}Y \right)$，然后将 $dY$ 写成 $dX$ 进行带入并使用迹技巧将其他项交换到 $dX$ 左侧，从而解决问题。

一个常见的情形是 $Y=AXB$，此时：

$$
\begin{array}{c}
	\mathrm{d}f=\mathrm{tr}\left( \frac{\partial f}{\partial Y}^T\mathrm{d}Y \right) =\mathrm{tr}\left( \frac{\partial f}{\partial Y}^TA\mathrm{d}XB \right)\\
	=\mathrm{tr}\left( B\frac{\partial f}{\partial Y}^TA\mathrm{d}X \right) =\mathrm{tr}\left( \left( A^T\frac{\partial f}{\partial Y}B^T \right) ^T\mathrm{d}X \right)\\
\end{array}
$$


从而 $\frac{\partial f}{\partial X}=A^T\frac{\partial f}{\partial Y}B^T$。注意这里 $\mathrm{d}Y=\mathrm{d}AXB+A\mathrm{d}XB+AX\mathrm{d}B=A\mathrm{d}XB$。

下面演示一些算例。

#### Case 1

::: warning problem
$f=a^TXb$，其中a是mx1的列向量，X是mxn的矩阵，b是nx1的列向量。
:::




$$
\begin{array}{c}
	\mathrm{d}f=\mathrm{d}a^TXb+a^T\mathrm{d}Xb+a^TX\mathrm{d}b=a^T\mathrm{d}Xb\\
	\Rightarrow \mathrm{d}f=\mathrm{tr}\left( \mathrm{d}f \right) =\mathrm{tr}\left( a^T\mathrm{d}Xb \right) =\mathrm{tr}\left( \left( ab^T \right) ^T\mathrm{d}X \right)\\
\end{array}
$$


因此求导结果为：


$$
\frac{\partial f}{\partial X}=ab^T
$$


#### Case 2
::: warning problem
$f=a^T\exp \left( Xb \right)$，其中a是mx1的列向量，X是mxn的矩阵，b是nx1的列向量。exp表示逐元素求指数。
:::



$$
\begin{array}{c}
	\mathrm{d}f=a^T\left( \exp \left( Xb \right) \odot \mathrm{d}\left( Xb \right) \right)\\
	\mathrm{d}f=\mathrm{tr}\left( a^T\left( \exp \left( Xb \right) \odot \mathrm{d}\left( Xb \right) \right) \right)\\
	=\mathrm{tr}\left( \left( a\odot \exp \left( Xb \right) \right) ^T\mathrm{d}\left( Xb \right) \right)\\
	=\mathrm{tr}\left( b\left( a\odot \exp \left( Xb \right) \right) ^T\mathrm{d}\left( X \right) \right)\\
	=\mathrm{tr}\left( \left( a\odot \exp \left( Xb \right) b^T \right) ^T\mathrm{d}\left( X \right) \right)\\
\end{array}
$$


这表明


$$
\frac{\partial f}{\partial X}=a\odot \exp \left( Xb \right) b^T
$$


#### Case 3

::: warning problem
$f=\mathrm{tr}\left( Y^TMY \right) , Y=\sigma \left( WX \right)$，其中W是lxm矩阵，X是mxn矩阵，Y是lxn矩阵，M是lxl对称矩阵，σ是逐元素函数。
:::




$$
\begin{array}{c}
	\mathrm{d}f=\mathrm{tr}\left( \left( \mathrm{d}Y \right) ^TMY+Y^TM\mathrm{d}Y \right)\\
	=\mathrm{tr}\left( \left( \mathrm{d}Y \right) ^TMY \right) +\mathrm{tr}\left( Y^TM\mathrm{d}Y \right)\\
	=\mathrm{tr}\left( Y^TM^T\mathrm{d}Y \right) +\mathrm{tr}\left( Y^TM\mathrm{d}Y \right)\\
	=\mathrm{tr}\left( Y^T\left( M+M^T \right) \mathrm{d}Y \right)\\
\end{array}
$$


这表明 $\frac{\partial f}{\partial Y}=\left( M+M^T \right) Y=2MY$

另外，有：


$$
\begin{array}{c}
	\mathrm{d}f=\mathrm{tr}\left( \frac{\partial f}{\partial Y}^T\mathrm{d}Y \right) =\mathrm{tr}\left( \frac{\partial f}{\partial Y}^T\left( \sigma '\left( WX \right) \odot \left( W\mathrm{d}X \right) \right) \right)\\
	=\mathrm{tr}\left( \left( \frac{\partial f}{\partial Y}\odot \sigma '\left( WX \right) \right) ^TW\mathrm{d}X \right)\\
\end{array}
$$


从而有：


$$
\begin{array}{c}
	\frac{\partial f}{\partial X}=W^T\left( \frac{\partial f}{\partial Y}\odot \sigma '\left( WX \right) \right)\\
	=W^T\left( 2M\sigma \left( WX \right) \odot \sigma '\left( WX \right) \right)\\
\end{array}
$$


#### Case 4- 线性回归

::: warning problem
$l=\left\| Xw-y \right\| ^2$，求w的最小二乘估计，也就是求$\frac{\partial l}{\partial w}$的零点。其中y是mx1的列向量，X是mxn矩阵，w是nx1列向量。
:::




$$
\begin{array}{c}
	l=\left\| Xw-y \right\| ^2=\left( Xw-y \right) ^T\left( Xw-y \right)\\
	\mathrm{d}l=\left( X\mathrm{d}w \right) ^T\left( Xw-y \right) +\left( Xw-y \right) ^TX\mathrm{d}w\\
	=2\left( Xw-y \right) ^TX\mathrm{d}w\\
\end{array}
$$


从而 
$$
\frac{\partial l}{\partial w}=2X^T\left( Xw-y \right) 
$$


因此最小二乘估计满足：


$$
\begin{array}{c}
	\frac{\partial l}{\partial w}=2X^T\left( Xw-y \right) =0\\
	w=\left( X^TX \right) ^{-1}X^Ty\\
\end{array}
$$


#### case5- 方差的最大似然估计

::: warning problem
样本$x_1,\cdots ,x_n\sim N\left( \mu ,\sigma \right)$，求方差$\sigma$的最大似然估计，即：$l=\log \left( \left| \sigma \right| \right) +\frac{1}{N}\sum_i{\left( x_i-\bar{x} \right) ^T\sigma ^{-1}\left( x_i-\bar{x} \right)}$, 求$\frac{\partial l}{\partial \sigma}$的零点。其中xi是mx1列向量，$\bar{x}=\frac{1}{N}\sum_i{x_i}$是样本均值，$\sigma$是mxm对称正定矩阵，l是标量。

:::


首先计算 $\mathrm{d}\log \left( \left| \sigma \right| \right) =\left| \sigma \right|^{-1}\mathrm{d}\left| \sigma \right|=\mathrm{tr}\left( \sigma ^{-1}\mathrm{d}\sigma \right)$，这里使用了前面 det 的微分形式。
对第二项有，

$$
\begin{array}{c}
	\mathrm{d}\left( \frac{1}{N}\sum_i{\left( x_i-\bar{x} \right) ^T\sigma ^{-1}\left( x_i-\bar{x} \right)} \right) =\frac{1}{N}\sum_i{\left( x_i-\bar{x} \right) ^T\mathrm{d}\sigma ^{-1}\left( x_i-\bar{x} \right)}\\
	=\frac{1}{N}\sum_i{\left( x_i-\bar{x} \right) ^T\left( -\sigma ^{-1}\left( \mathrm{d}\sigma \right) \sigma ^{-1} \right) \left( x_i-\bar{x} \right)}\\
	=-\mathrm{tr}\left( \frac{1}{N}\sum_i{\left( x_i-\bar{x} \right) ^T\left( \sigma ^{-1}\left( \mathrm{d}\sigma \right) \sigma ^{-1} \right) \left( x_i-\bar{x} \right)} \right)\\
	=-\frac{1}{N}\sum_i{\mathrm{tr}\left( \left( x_i-\bar{x} \right) ^T\left( \sigma ^{-1}\left( \mathrm{d}\sigma \right) \sigma ^{-1} \right) \left( x_i-\bar{x} \right) \right)}\\
	=-\frac{1}{N}\sum_i{\mathrm{tr}\left( \sigma ^{-1}\left( x_i-\bar{x} \right) \left( x_i-\bar{x} \right) ^T\sigma ^{-1}\left( \mathrm{d}\sigma \right) \right)}\\
	=-\mathrm{tr}\left( \sigma ^{-1}S\sigma ^{-1}\mathrm{d}\sigma \right)\\
\end{array}
$$


其中 $S=\frac{1}{N}\sum_i{\left( x_i-\bar{x} \right) \left( x_i-\bar{x} \right) ^T}$ 是样本方差矩阵。因此综上可以得到：


$$
\frac{\partial l}{\partial \sigma}=\left( \sigma ^{-1}-\sigma ^{-1}S\sigma ^{-1} \right) ^T
$$


因此零点满足 $\sigma=S$，也即其最大似然估计。

#### case6- 多元 Logistic 回归

::: warning problem
$l=-y^T\log \left( \mathrm{soft}\max \left( Wx \right) \right)$，其中y是除了一个元素为1外其他元素为0的mx1列向量，W是mxn矩阵，x是nx1列向量。$\mathrm{soft}\max \left( a \right) =\frac{\exp \left( a \right)}{1^T\exp \left( a \right)}$。

:::


方法 1


$$
\begin{array}{c}
	l=-y^T\log \left( \mathrm{soft}\max \left( Wx \right) \right) =-y^T\left( \log \left( \exp \left( Wx \right) \right) -1\log \left( 1^T\exp \left( Wx \right) \right) \right)\\
	=-y^TWx+\log \left( 1^T\exp \left( Wx \right) \right)\\
\end{array}
$$

进一步进行微分：


$$
\begin{array}{c}
	\mathrm{d}l=-y^T\mathrm{d}Wx+\frac{1^T\left( \exp \left( Wx \right) \odot \mathrm{d}\left( Wx \right) \right)}{1^T\exp \left( Wx \right)}\\
	=\mathrm{tr}\left( -y^T\mathrm{d}Wx+\frac{1^T\left( \exp \left( Wx \right) \odot \mathrm{d}\left( Wx \right) \right)}{1^T\exp \left( Wx \right)} \right)\\
	=\mathrm{tr}\left( -y^T\mathrm{d}Wx \right) +\frac{\mathrm{tr}\left( 1^T\left( \exp \left( Wx \right) \odot \mathrm{d}\left( Wx \right) \right) \right)}{1^T\exp \left( Wx \right)}\\
	=\mathrm{tr}\left( -y^T\mathrm{d}Wx \right) +\frac{\mathrm{tr}\left( \exp \left( Wx \right) ^T\mathrm{d}Wx \right)}{1^T\exp \left( Wx \right)}\\
	=\mathrm{tr}\left( -y^T\mathrm{d}Wx+\frac{\exp \left( Wx \right) ^T\mathrm{d}Wx}{1^T\exp \left( Wx \right)} \right)\\
	=\mathrm{tr}\left( -y^T\mathrm{d}Wx+\mathrm{soft}\max \left( Wx \right) ^T\mathrm{d}Wx \right)\\
	=\mathrm{tr}\left( x\left( \mathrm{soft}\max \left( Wx \right) -y \right) ^T\mathrm{d}W \right)\\
\end{array}
$$


因此 $\frac{\partial l}{\partial W}=\left( \mathrm{soft}\max \left( Wx \right) -y \right) x^T$

方法 2

定义 $a=Wx$，那么 $l=-y^T\log \left( \mathrm{soft}\max \left( a \right) \right)$

类似上面思路求出 $\frac{\partial l}{\partial a}=\mathrm{soft}\max \left( a \right) -y$，那么利用复合法则，有：


$$
\mathrm{d}l=\mathrm{tr}\left( \frac{\partial l}{\partial a}^T\mathrm{d}a \right) =\mathrm{tr}\left( \frac{\partial l}{\partial a}^T\mathrm{d}Wx \right) =\mathrm{tr}\left( x\frac{\partial l}{\partial a}^T\mathrm{d}W \right) 
$$


从而可以得到：


$$
\frac{\partial l}{\partial W}=\frac{\partial l}{\partial a}x^T
$$




#### case7- 二层神经网络

::: warning problem
$l=-y^T\log \left( \mathrm{soft}\max \left( W_2\sigma \left( W_1x \right) \right) \right)$，求$\frac{\partial l}{\partial W_1}, \frac{\partial l}{\partial W_2}$。其中y是一个除一个元素为1以外其余元素全为0的mx1列向量，W2是mxp矩阵，W1是pxn矩阵，$\mathrm{soft}\max \left( a \right) =\frac{\exp \left( a \right)}{1^T\exp \left( a \right)}$，σ是逐元素sigmoid函数$\sigma \left( a \right) =\frac{1}{1+\exp \left( -a \right)}$
:::


定义 $a_1=W_1x, h_1=\sigma \left( a_1 \right) , a_2=W_2h_1$，那么有


$$
\begin{array}{c}
	l=-y^T\log \left( \mathrm{soft}\max \left( a_2 \right) \right) , \frac{\partial l}{\partial a_2}=\mathrm{soft}\max \left( a_2 \right) -y\\
	\Rightarrow \mathrm{d}l=\mathrm{tr}\left( \frac{\partial l}{\partial a_2}^T\mathrm{d}a_2 \right) ={\color{red}\mathrm{tr}\left( \frac{\partial l}{\partial a_2}^T\mathrm{d}W_2h_1 \right)}+\underset{\mathrm{d}l_2}{\underbrace{\mathrm{tr}\left( \frac{\partial l}{\partial a_2}^TW_2\mathrm{d}h_1 \right) }}\\
	\Rightarrow {\color{red}\frac{\partial l}{\partial W_2}=\frac{\partial l}{\partial a_2}h_{1}^{T}}, \frac{\partial l}{\partial h_1}=W_{2}^{T}\frac{\partial l}{\partial a_2}\\
\end{array}
\\

$$


对第二项进一步分析：


$$
\begin{array}{c}
	\mathrm{d}l_2=\mathrm{tr}\left( \frac{\partial l}{\partial h_1}^T\mathrm{d}h_1 \right) =\mathrm{tr}\left( \frac{\partial l}{\partial h_1}^T\left( \sigma '\left( a_1 \right) \odot \mathrm{d}a_1 \right) \right)\\
	=\mathrm{tr}\left( \left( \frac{\partial l}{\partial h_1}\odot \sigma '\left( a_1 \right) \right) ^T\mathrm{d}a_1 \right)\\
	\Rightarrow \frac{\partial l_2}{\partial a_1}=\frac{\partial l}{\partial h_1}\odot \sigma '\left( a_1 \right)\\
\end{array}
$$


进一步：


$$
\mathrm{d}l_2=\mathrm{tr}\left( \frac{\partial l}{\partial a_1}^T\mathrm{d}a_1 \right) =\mathrm{tr}\left( \frac{\partial l}{\partial a_1}^T\mathrm{d}W_1x \right)
\\
\Rightarrow \frac{\partial l}{\partial W_1}=\frac{\partial l}{\partial a_1}x^T
$$


::: warning problem
对问题可以更进一步，样本满足：

$$
\begin{array}{c}
	\left( x_1,y_1 \right) ,\cdots ,\left( x_N,y_N \right) ,\\
	l=-\sum_i{y_{i}^{T}\log \left( \mathrm{soft}\max \left( W_2\sigma \left( W_1x_i+b_1 \right) +b_2 \right) \right)}\\
\end{array}
$$

其中b1是px1列向量，b2是mx1列向量

:::


类似上面思路，定义 $a_{1,i}=W_1x_i+b_1, h_{1,i}=\sigma \left( a_{i,i} \right) , a_{2,i}=W_2h_{1,i}+b_2$

从而问题转换为：


$$
l=-\sum_i{y_{i}^{T}\log \left( \mathrm{soft}\max \left( a_{2,i} \right) \right)}, \frac{\partial l}{\partial a_{2,i}}=\mathrm{soft}\max \left( a_{2,i} \right) -y_i
$$


类似使用复合法则，有：


$$
\begin{array}{c}
	\mathrm{d}l=\mathrm{tr}\left( \sum_i{\frac{\partial l}{\partial a_{2,i}}^T\mathrm{d}a_{2,i}} \right) =\\
	\mathrm{tr}\left( \sum_i{\frac{\partial l}{\partial a_{2,i}}^T\mathrm{d}W_2h_{1,i}} \right) +\underset{\mathrm{d}l_2}{\underbrace{\mathrm{tr}\left( \sum_i{\frac{\partial l}{\partial a_{2,i}}^TW_2\mathrm{d}h_{1,i}} \right) }}+\mathrm{tr}\left( \sum_i{\frac{\partial l}{\partial a_{2,i}}^T\mathrm{d}b_2} \right)\\
\end{array}
$$

从第一项有：


$$
\frac{\partial l}{\partial W_2}=\sum_i{\frac{\partial l}{\partial a_{2,i}}h_{1,i}^{T}}
$$


从第二项有：


$$
\frac{\partial l}{\partial h_{1,i}}=W_{2}^{T}\frac{\partial l}{\partial a_{2,i}}
$$


从第三项有：


$$
\frac{\partial l}{\partial b_2}=\sum_i{\frac{\partial l}{\partial a_{2,i}}}
$$


对第二项进一步使用复合法则有：


$$
\frac{\partial l}{\partial a_{1,i}}=\frac{\partial l}{\partial h_{1,i}}\odot \sigma '\left( a_{1,i} \right) 
$$



$$
\begin{array}{c}
	\mathrm{d}l_2=\mathrm{tr}\left( \sum_i{\frac{\partial l}{\partial a_{1,i}}^T\mathrm{d}a_{1,i}} \right)\\
	=\mathrm{tr}\left( \sum_i{\frac{\partial l}{\partial a_{1,i}}^T\mathrm{d}W_1x_i} \right) +\mathrm{tr}\left( \sum_i{\frac{\partial l}{\partial a_{1,i}}^T\mathrm{d}b_1} \right)\\
	\Rightarrow \frac{\partial l_2}{\partial W_1}=\sum_i{\frac{\partial l}{\partial a_{1,i}}{x_i}^T}, \frac{\partial l}{\partial b_1}=\sum_i{\frac{\partial l}{\partial a_{1,i}}}\\
\end{array}
$$


---
另一个求解思路如下：

使用矩阵表示 N 个样本来简化形式，定义


$$
\begin{array}{c}
	X=\left[ x_1,\cdots ,x_N \right] ,A_1=\left[ a_{1,1},\cdots ,a_{1,N} \right] =W_1X+b_11^T\\
	H_1=\left[ h_{1,1},\cdots ,h_{1,N} \right] =\sigma \left( A_1 \right) ,A_2=W_2H_1+b_21^T\\
\end{array}
$$

注意上面的 1 是用来扩展维度的。

类似上面的思路求出：


$$
\frac{\partial l}{\partial A_2}=\left[ \mathrm{soft}\max \left( a_{2,1} \right) -y_1,\cdots \mathrm{soft}\max \left( a_{2,N} \right) -y_N \right] 
$$


使用复合法则有：


$$
\begin{array}{c}
	\mathrm{d}l=\mathrm{tr}\left( \frac{\partial l}{\partial A_2}^T\mathrm{d}A_2 \right)\\
	=\mathrm{tr}\left( \frac{\partial l}{\partial A_2}^T\mathrm{d}W_2H_1 \right) +\underset{\mathrm{d}l_2}{\underbrace{\mathrm{tr}\left( \frac{\partial l}{\partial A_2}^TW_2\mathrm{d}H_1 \right) }}+\mathrm{tr}\left( \frac{\partial l}{\partial A_2}^T\mathrm{d}b_21^T \right)\\
\end{array}
$$


从而得到了：


$$
\frac{\partial l}{\partial W_2}=\frac{\partial l}{\partial A_2}H_{1}^{T}, \frac{\partial l}{\partial H_1}=W_{2}^{T}\frac{\partial l}{\partial A_2}, \frac{\partial l}{\partial b_2}=\frac{\partial l}{\partial A_2}1
$$


对第二项使用复合法则继续有：


$$
\begin{array}{c}
	\mathrm{d}l_2=\mathrm{tr}\left( \frac{\partial l}{\partial A_1}^T\mathrm{d}A_1 \right) =\mathrm{tr}\left( \frac{\partial l}{\partial A_1}^T\mathrm{d}W_1X \right) +\mathrm{tr}\left( \frac{\partial l}{\partial A_1}^T\mathrm{d}b_11^T \right)\\
	\Rightarrow \frac{\partial l_2}{\partial W_1}=\frac{\partial l_2}{\partial A_1}X^T, \frac{\partial l_2}{\partial b_1}=\frac{\partial l_2}{\partial A_1}1\\
\end{array}
$$


### 矩阵对矩阵的求导

上面分析了标量对矩阵的导数，下面来分析下矩阵对矩阵的求导。

首先分析下矩阵对矩阵的导数应该有怎么样的定义：
- 矩阵 F(pxq) 对矩阵 X(mxn) 的导数应该有所有 mnpq 个偏导数 $\frac{\partial F_{kl}}{\partial X_{ij}}$
- 导数和微分之间应该有类似上面标量微分的关系

根据上面的思考，我们首先定义向量 f(px1) 对向量 x(mx1) 的导数：


$$
\frac{\partial f}{\partial x}=\left( \begin{matrix}
	\frac{\partial f_1}{\partial x_1}&		\frac{\partial f_2}{\partial x_1}&		\cdots&		\frac{\partial f_p}{\partial x_1}\\
	\frac{\partial f_1}{\partial x_2}&		\frac{\partial f_2}{\partial x_2}&		\cdots&		\frac{\partial f_p}{\partial x_2}\\
	\vdots&		\vdots&		\ddots&		\vdots\\
	\frac{\partial f_1}{\partial x_m}&		\frac{\partial f_2}{\partial x_m}&		\cdots&		\frac{\partial f}{\partial x_m}\\
\end{matrix} \right) 
$$


其中结果是一个 mxp 的矩阵，满足 ${\color{red}\mathrm{d}f=\frac{\partial f}{\partial x}^T\mathrm{d}x}$。

接下来定义矩阵的 (**按列优先**) 向量化，即：


$$
\mathrm{vec}\left( X \right) =\left[ X_{11},\cdots ,X_{m1},X_{12},\cdots ,X_{m2},\cdots ,X_{1n},\cdots ,X_{mn} \right] ^T
$$


::: tip definition
定义矩阵F对矩阵X的导数满足：


$$
\frac{\partial F}{\partial X}=\frac{\partial \mathrm{vec}\left( F \right)}{\partial \mathrm{vec}\left( X \right)}\left( \mathrm{dim}: mn\times pq \right) 
$$


微分和导数之间具有联系：


$$
\mathrm{vec}\left( \mathrm{d}F \right) =\frac{\partial F}{\partial X}^T\mathrm{vec}\left( \mathrm{d}X \right) 
$$

:::



::: note note
- 按照这个定义，标量f对矩阵X(mxn)的矩阵是mnx1的向量，和上面标量的定义之间存在矛盾，不过这里比较容易相互转换。为了避免混淆，这里使用$\nabla _Xf$表示上面定义的mxn矩阵，从而有：$\frac{\partial f}{\partial X}=\mathrm{vec}\left( \nabla _Xf \right)$
- 标量对矩阵的二阶导数，称为Hessian矩阵，定义为：
$$
\nabla _{X}^{2}f=\frac{\partial ^2f}{\partial X^2}=\frac{\partial \nabla _Xf}{\partial X}\left( \mathrm{dim}: mn\times mn \right) 
$$
是对称矩阵，对向量$\frac{\partial{f}}{\partial{X}}$或者矩阵$\nabla _Xf$求导都可以得到结果，但从矩阵出发更加简单
- $\frac{\partial F}{\partial X}=\frac{\partial \mathrm{vec}\left( F \right)}{\partial X}=\frac{\partial F}{\partial \mathrm{vec}\left( X \right)}=\frac{\partial \mathrm{vec}\left( F \right)}{\partial \mathrm{vec}\left( X \right)}$，从而导致求导时矩阵被向量化，破坏了矩阵的整体结构，结果形式变得复杂；但好处是多元微积分中关于梯度、Hessian矩阵的结论可以沿用过来，只需要矩阵向量化即可。比如优化问题中牛顿法的更新$\Delta X$，满足
$$
\frac{\partial F}{\partial X}=\frac{\partial \mathrm{vec}\left( F \right)}{\partial X}=\frac{\partial F}{\partial \mathrm{vec}\left( X \right)}=\frac{\partial \mathrm{vec}\left( F \right)}{\partial \mathrm{vec}\left( X \right)}
$$

- 在资料中，矩阵对矩阵的导数还有其他的定义，比如
$$
\begin{array}{c}
	\frac{\partial F}{\partial X}=\left[ \frac{\partial F_{kl}}{\partial X} \right] \left( \mathrm{dim}: mp\times nq \right)\\
	\frac{\partial F}{\partial X}=\left[ \frac{\partial F}{\partial X_{ij}} \right] \left( \mathrm{dim}: mp\times nq \right)\\
\end{array}
$$

这两种定义可以兼容上面标量对矩阵导数的定义，但是**微分和导数的联系**（$dF$等于导数中逐个mxn子块分别与dX做内积)意义不够简明，不便于计算和应用。
- 在资料中，有分子布局和分母布局两种定义，向量对向量的导数的排布有所不同，这里使用的是**分母布局**，机器学习和优化中的梯度矩阵采用此定义。**而控制论等领域中的Jacobian矩阵采用分子布局**。
	- 分母布局：
$$
\frac{\partial f}{\partial x}=\left( \begin{matrix}
	\frac{\partial f_1}{\partial x_1}&		\frac{\partial f_2}{\partial x_1}&		\cdots&		\frac{\partial f_p}{\partial x_1}\\
	\frac{\partial f_1}{\partial x_2}&		\frac{\partial f_2}{\partial x_2}&		\cdots&		\frac{\partial f_p}{\partial x_2}\\
	\vdots&		\vdots&		\ddots&		\vdots\\
	\frac{\partial f_1}{\partial x_m}&		\frac{\partial f_2}{\partial x_m}&		\cdots&		\frac{\partial f}{\partial x_m}\\
\end{matrix} \right) 
$$

	- 分子布局：
$$
\frac{\partial f}{\partial x}=\left( \begin{matrix}
	\frac{\partial f_1}{\partial x_1}&		\frac{\partial f_1}{\partial x_2}&		\cdots&		\frac{\partial f_1}{\partial x_m}\\
	\frac{\partial f_2}{\partial x_1}&		\frac{\partial f_2}{\partial x_2}&		\cdots&		\frac{\partial f_2}{\partial x_m}\\
	\vdots&		\vdots&		\vdots&		\vdots\\
	\frac{\partial f_n}{\partial x_1}&		\frac{\partial f_n}{\partial x_2}&		\cdots&		\frac{\partial f_n}{\partial x_m}\\
	\end{matrix} \right) 
$$

	- 分子布局下，相应的导数和微分的关系转变成了
$$
\mathrm{d}f=\frac{\partial f}{\partial x}\mathrm{d}x, \frac{\partial F}{\partial X}=\frac{\partial \mathrm{vec}\left( F \right)}{\partial \mathrm{vec}\left( X \right)}, \mathrm{vec}\left( \mathrm{d}F \right) =\frac{\partial F}{\partial X}\mathrm{vec}\left( \mathrm{d}X \right) 
$$

	- 这两者互为转置，并无大的区别

:::


类似标量求导的思路，现在建立运算法则。

依然使用导数和微分的联系，$\mathrm{vec}\left( \mathrm{d}F \right) =\frac{\partial F}{\partial X}^T\mathrm{vec}\left( \mathrm{d}X \right)$。求微分的思路和上面相同，但是需要一些向量化的技巧。

- 线性：$\mathrm{vec}\left( A+B \right) =\mathrm{vec}\left( A \right) +\mathrm{vec}\left( B \right)$
- 矩阵乘法: $\mathrm{vec}\left( AXB \right) =\left( B^T\otimes A \right) \mathrm{vec}\left( X \right)$
	- 其中 $\otimes$ 代表 kronecker 积，A(mxn) 和 B(pxq) 的 Kronecker 积为：
$$
A\otimes B=\left[ A_{ij}B \right] \left( \mathrm{dim}: mp\times nq \right) 
$$

	- 此式证明见张贤达《矩阵分析与应用》第 107-108 页。
- 转置：$\mathrm{vec}\left( A^T \right) =K_{mn}\mathrm{vec}\left( A \right)$，其中 A 是 mxn 矩阵
	- Kmn(mnxmn) 是交换矩阵 (commutation matrix)，将按列优先的向量化变为按行优先的向量化
	- 
$$
K_{22}=\left( \begin{matrix}
	1&		&		&		\\
	&		&		1&		\\
	&		1&		&		\\
	&		&		&		1\\
\end{matrix} \right) , \mathrm{vec}\left( A^T \right) =\left( \begin{array}{c}
	A_{11}\\
	A_{12}\\
	A_{21}\\
	A_{22}\\
\end{array} \right) , \mathrm{vec}\left( A \right) =\left( \begin{array}{c}
	A_{11}\\
	A_{21}\\
	A_{12}\\
	A_{22}\\
\end{array} \right) 
$$

- 逐元素乘法：$\mathrm{vec}\left( A\odot X \right) =\mathrm{diag}\left( A \right) \mathrm{vec}\left( X \right)$
	- $\mathrm{diag}\left( A \right)$ 是使用 A 的元素 (按列优先) 排成的对角阵

相比于标量的导数，矩阵的导数显得更加复杂，这也是应该的，因为我们将一个本来是 4 维的向量投射到 2 维来进行运算，能保持运算就不错了：(

此外，考虑复合的情况。假设已知 $\frac{\partial F}{\partial Y}$，Y 是 X 的函数，从导数和微分的联系入手有：


$$
\begin{array}{c}
	\mathrm{vec}\left( \mathrm{d}F \right) =\frac{\partial F}{\partial Y}^T\mathrm{vec}\left( \mathrm{d}Y \right) =\frac{\partial F}{\partial Y}^T\frac{\partial Y}{\partial X}^T\mathrm{vec}\left( \mathrm{d}X \right)\\
	\Rightarrow {\color{red}\frac{\partial F}{\partial X}=\frac{\partial Y}{\partial X}\frac{\partial F}{\partial Y}}\\
\end{array}
$$


注意上面式子是依赖于选取的分子/分母形式的，分子布局的应为下面形式：

$$
\begin{array}{c}
	\mathrm{vec}\left( \mathrm{d}F \right) =\frac{\partial F}{\partial Y}\mathrm{vec}\left( \mathrm{d}Y \right) =\frac{\partial F}{\partial Y}\frac{\partial Y}{\partial X}\mathrm{vec}\left( \mathrm{d}X \right)\\
	\Rightarrow \frac{\partial F}{\partial X}=\frac{\partial F}{\partial Y}\frac{\partial Y}{\partial X}\\
\end{array}
$$


此外，这里有一些关系 Kronecker 积的恒等式：
- $\left( A\otimes B \right) ^T=A^T\otimes B^T$
- $\mathrm{vec}\left( ab^T \right) =b\otimes a$
- $\left( A\otimes B \right) \left( C\otimes D \right) =\left( AC \right) \otimes \left( BD \right)$
	- 这可以通过对 $F=D^TB^TXAC$ 求导证明
		- 直接求导有：$\frac{\partial F}{\partial X}=\left( AC \right) \otimes \left( BD \right)$
		- 另一方面：
$$
\begin{array}{c}
	Y=B^TXA\Rightarrow \frac{\partial F}{\partial Y}=C\otimes D\\
	\frac{\partial Y}{\partial X}=A\otimes B\Rightarrow \frac{\partial F}{\partial X}=\left( A\otimes B \right) \left( C\otimes D \right)\\
\end{array}
$$

- $K_{mn}=K_{nm}^{T}, K_{mn}K_{nm}=I$
- $K_{pm}\left( A\otimes B \right) K_{nq}=B\otimes A$，其中 A 是 mxn 矩阵，B 是 pxq 矩阵
	- 可以对 $AXB^T$ 进行分析得到
		- 
$$
\begin{array}{c}
	\mathrm{vec}\left( AXB^T \right) =\left( B\otimes A \right) \mathrm{vec}\left( X \right)\\
	\mathrm{vec}\left( AXB^T \right) =K_{pm}\mathrm{vec}\left( BX^TA^T \right) =K_{pm}\left( A\otimes B \right) K_{nq}\mathrm{vec}\left( X \right)\\
\end{array}
$$

#### case1

::: warning problem
$F=AX$，其中X是mxn矩阵，求$\frac{\partial F}{\partial X}$
:::



$$
\begin{array}{c}
	\mathrm{d}F=A\mathrm{d}X\Rightarrow \mathrm{vec}\left( \mathrm{d}F \right) =\mathrm{vec}\left( A\mathrm{d}X \right)\\
	\Rightarrow \mathrm{vec}\left( \mathrm{d}F \right) =\left( I_n\otimes A \right) \mathrm{vec}\left( \mathrm{d}X \right)\\
	\Rightarrow \mathrm{d}F=\left( I_n\otimes A \right) ^T\\
\end{array}
$$


如果 X 退化为向量，那么有：


$$
f=Ax, \mathrm{d}f=\frac{\partial f}{\partial x}^T\mathrm{d}x\Rightarrow \frac{\partial f}{\partial x}=A^T
$$


#### case2

::: warning problem
$f=\log \left| X \right|$，X是nxn矩阵，求$\nabla _Xf$和$\nabla _{X}^{2}f$
:::


使用上面标量计算的结果，$\nabla _Xf=X^{-1T}$。为了计算 $\nabla _{X}^{2}f$，首先考虑微分：


$$
\begin{array}{c}
	\mathrm{d}\nabla _Xf=-\left( X^{-1}\mathrm{d}XX^{-1} \right) ^T\\
	\Rightarrow \mathrm{vec}\left( \mathrm{d}\nabla _Xf \right) =-K_{nn}\mathrm{vec}\left( X^{-1}\mathrm{d}XX^{-1} \right)\\
	=-K_{nn}\left( X^{-1T}\otimes X^{-1} \right) \mathrm{vec}\left( \mathrm{d}X \right)\\
\end{array}
$$


从而结果为：

$$
\nabla _{X}^{2}f=-K_{nn}\left( X^{-1T}\otimes X^{-1} \right) 
$$


这里注意上面是一个对称矩阵，所以消掉了转置。

当 X 是对称矩阵时，有：


$$
\nabla _{X}^{2}f=-\left( X^{-1}\otimes X^{-1} \right) 
$$


#### case3

::: warning problem
$F=A\exp \left( XB \right)$，A是lxm矩阵，X是mxn矩阵，B是nxp矩阵，exp是逐元素函数。
:::



$$
\begin{array}{c}
	\mathrm{d}F=A\left( \exp \left( XB \right) \odot \left( \mathrm{d}XB \right) \right)\\
	\Rightarrow \mathrm{vec}\left( \mathrm{d}F \right) =\mathrm{vec}\left( A\left( \exp \left( XB \right) \odot \left( \mathrm{d}XB \right) \right) \right)\\
	=\left( I_p\otimes A \right) \mathrm{vec}\left( \exp \left( XB \right) \odot \left( \mathrm{d}XB \right) \right)\\
	=\left( I_p\otimes A \right) \mathrm{diag}\left( \exp \left( XB \right) \right) \mathrm{vec}\left( \mathrm{d}XB \right)\\
	=\left( I_p\otimes A \right) \mathrm{diag}\left( \exp \left( XB \right) \right) \left( B^T\otimes I_m \right) \mathrm{vec}\left( \mathrm{d}X \right)\\
\end{array}
$$


从而有：


$$
\frac{\partial F}{\partial X}=\left( B\otimes I_m \right) \mathrm{diag}\left( \exp \left( XB \right) \right) \left( I_p\otimes A^T \right) 
$$


#### case4- 一元 Logistic 回归

::: warning problem
$l=-yx^Tw+\log \left( 1+\exp \left( x^Tw \right) \right)$，求$\nabla _wl, \nabla _{w}^{2}l$
:::


使用上面标量乘法的结论，有 $\nabla _wl=x\left( \sigma \left( x^Tw \right) -y \right) , \sigma \left( a \right) =\frac{\exp \left( a \right)}{1+\exp \left( a \right)}$

为了求 $\nabla _{w}^{2}l$，首先考虑微分


$$
\begin{array}{c}
	\mathrm{d}\nabla _wl=\mathrm{d}\left( x\left( \sigma \left( x^Tw \right) -y \right) \right) =x\sigma '\left( x^Tw \right) x^T\mathrm{d}w\\
	\sigma '\left( a \right) =\frac{\exp \left( a \right)}{\left( 1+\exp \left( a \right) \right) ^2}\\
\end{array}
$$


按照导数和微分的关系，得到：


$$
\nabla _{w}^{2}l=x\sigma '\left( x^Tw \right) x^T
$$


推广问题：

::: warning problem
$\left( x_1,y_1 \right) ,\cdots ,\left( x_N,y_N \right) , l=\sum_i{\left( -y_ix_{i}^{T}w+\log \left( 1+\exp \left( x_{i}^{T}w \right) \right) \right)}$，求$\nabla _wl, \nabla _{w}^{2}l$
:::


解法 1：先对每个样本求导，然后相加

解法 2：定义矩阵 
$$
X=\left( \begin{array}{c}
	x_{1}^{T}\\
	\vdots\\
	x_{N}^{T}\\
\end{array} \right) , y=\left( \begin{array}{c}
	y_1\\
	\vdots\\
	y_N\\
\end{array} \right) 
$$

将 l 写成矩阵形式有 $l=-y^TXw+1^T\log \left( 1+\exp \left( Xw \right) \right)$
从而有：


$$
\begin{array}{c}
	\nabla _wl=X^T\left( \sigma \left( Xw \right) -y \right)\\
	\mathrm{d}\nabla _wl=X^T\left( \sigma '\left( Xw \right) \odot \left( X\mathrm{d}w \right) \right)\\
	=X^T\mathrm{diag}\left( \sigma '\left( Xw \right) \right) X\mathrm{d}w\\
	\Rightarrow \nabla _{w}^{2}l=X^T\mathrm{diag}\left( \sigma '\left( Xw \right) \right) X\\
\end{array}
$$


#### case5- 多元 Logistic 回归

::: warning problem
$l=-y^T\log \left( \mathrm{soft}\max \left( Wx \right) \right) =-y^TWx+\log \left( 1^T\exp \left( Wx \right) \right)$，求$\nabla _Wl, \nabla _{W}^{2}l$
其中y是一个除一个元素为1外其它元素为0的mx1列向量，W是mxn矩阵，x是nx1列向量
:::


前面标量求导已经有


$$
\nabla _Wl=\left( \mathrm{soft}\max \left( Wx \right) -y \right) x^T
$$


进一步求


$$
\begin{array}{c}
	\mathrm{d}\nabla _Wl=\left( \frac{\exp \left( a \right) \odot \mathrm{d}a}{1^T\exp \left( a \right)}-\frac{\exp \left( a \right) \left( 1^T\left( \exp \left( a \right) \odot \mathrm{d}a \right) \right)}{\left( 1^T\exp \left( a \right) \right) ^2} \right) x^T\\
	=\left( \frac{\mathrm{diag}\left( \exp \left( a \right) \right)}{1^T\exp \left( a \right)}-\frac{\exp \left( a \right) \exp \left( a \right) ^T}{\left( 1^T\exp \left( a \right) \right) ^2} \right) \mathrm{d}ax^T\\
	=\left( \mathrm{diag}\left( \mathrm{soft}\max \left( a \right) \right) -\mathrm{soft}\max \left( a \right) \mathrm{soft}\max \left( a \right) ^T \right) \mathrm{d}ax^T\\
\end{array}
$$


注意这里化简去掉了逐元素乘法。
第一项 $\exp \left( a \right) \odot \mathrm{d}a=\mathrm{diag}\left( \exp \left( a \right) \right) \mathrm{d}a$
第二项 $1^T\left( \exp \left( a \right) \odot \mathrm{d}a \right) =\exp \left( a \right) ^T\mathrm{d}a$

进一步有：

$$
\begin{array}{c}
	D\left( a \right) =\mathrm{diag}\left( \mathrm{soft}\max \left( a \right) \right) -\mathrm{soft}\max \left( a \right) \mathrm{soft}\max \left( a \right) ^T\\
	\mathrm{d}\nabla _Wl=D\left( a \right) \mathrm{d}ax^T=D\left( Wx \right) \mathrm{d}Wxx^T\\
	\Rightarrow \nabla _{W}^{2}l=\left( xx^T \right) \otimes D\left( Wx \right)\\
\end{array}
$$


最后，进行一个总结。从 **整体** 出发的矩阵求导的技术，重点在于联系微分和导数。通过建立基本的微分形式和运算规则，可以得到想要求的导数结果。

- 对于标量对矩阵的微分，有 $\mathrm{d}f=\mathrm{tr}\left( \nabla _{X}^{T}f\mathrm{d}X \right)$
- 对于矩阵对矩阵的微分，有 $\mathrm{vec}\left( \mathrm{d}F \right) =\frac{\partial F}{\partial X}^T\mathrm{vec}\left( \mathrm{d}X \right)$


## 矩阵求导例子

### 标量求导


$$
\frac{d(u^Tv)}{dx} = \frac{d(u^T)}{dx} v+\frac{d(v^T)}{dx} u
$$


### 矢量求导

### 矩阵求导


## 参考

- [@petersenMatrixcookbook2021](.//)
- [矩阵求导术（上） - 知乎](https://zhuanlan.zhihu.com/p/24709748)
- [矩阵求导术（下） - 知乎](https://zhuanlan.zhihu.com/p/24863977)
